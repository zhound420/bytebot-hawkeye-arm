# Hawkeye Precision Features
BYTEBOT_GRID_OVERLAY=true
BYTEBOT_GRID_DEBUG=false
BYTEBOT_PROGRESSIVE_ZOOM_USE_AI=true
BYTEBOT_SMART_FOCUS=true
BYTEBOT_SMART_FOCUS_MODEL=gpt-4o-mini
BYTEBOT_UNIVERSAL_TEACHING=true
BYTEBOT_ADAPTIVE_CALIBRATION=true
BYTEBOT_ZOOM_REFINEMENT=true
BYTEBOT_COORDINATE_METRICS=true
BYTEBOT_COORDINATE_DEBUG=false

# Click Accuracy Settings
BYTEBOT_SMART_CLICK_SUCCESS_RADIUS=12
BYTEBOT_POST_CLICK_CALIBRATION=true
BYTEBOT_DRIFT_COMPENSATION=true
BYTEBOT_DRIFT_SMOOTHING=0.2
BYTEBOT_PRECLICK_SNAP=true
BYTEBOT_SNAP_RADIUS=6
BYTEBOT_SNAP_PENALTY=0.25
BYTEBOT_CLICK_RETRY_ON_NOCHANGE=true
BYTEBOT_CLICK_VERIFY_DELAY=250
BYTEBOT_CLICK_VERIFY_RADIUS=12
BYTEBOT_CLICK_VERIFY_THRESHOLD=4.0
BYTEBOT_CLICK_RETRY_MAX=1

# ==============================================================================
# Trajectory Distillation & Few-Shot Learning
# ==============================================================================
# Enable recording of successful task executions for learning
BYTEBOT_RECORD_TRAJECTORIES=true
# Models to record (comma-separated, empty = all)
BYTEBOT_RECORD_MODEL_PROVIDERS=anthropic,proxy
# Record failed tasks (for analysis)
BYTEBOT_RECORD_FAILURES=false
# Minimum task duration to record (seconds)
BYTEBOT_RECORD_MIN_DURATION=5

# Few-shot learning from successful trajectories
BYTEBOT_USE_FEW_SHOT=true
# Number of similar examples to inject
BYTEBOT_FEW_SHOT_COUNT=3
# Minimum similarity threshold (0.0-1.0)
BYTEBOT_FEW_SHOT_SIMILARITY=0.7
# Source providers for examples (comma-separated)
# Note: 'proxy' includes OpenRouter/LiteLLM runs, 'anthropic' for native Claude API
BYTEBOT_FEW_SHOT_SOURCE_PROVIDERS=anthropic,proxy

# Model-specific prompt adaptations
BYTEBOT_USE_MODEL_SPECIFIC_PROMPTS=true

# Quality thresholds
BYTEBOT_TRAJECTORY_QUALITY_THRESHOLD=0.7

# OpenAI API key for embeddings (can reuse OPENAI_API_KEY if available)
# BYTEBOT_EMBEDDING_API_KEY=

# ==============================================================================
# OmniParser Integration (Microsoft OmniParser v2.0)
# ==============================================================================
# Note: OMNIPARSER_URL is platform-specific
# - Apple Silicon: http://host.docker.internal:9989 (native with MPS GPU)
# - x86_64: http://bytebot-omniparser:9989 (Docker container)
# Default to host.docker.internal for Apple Silicon (most common ARM64 use case)
BYTEBOT_CV_USE_OMNIPARSER=true
OMNIPARSER_URL=http://host.docker.internal:9989
OMNIPARSER_TIMEOUT=30000

# Performance profile: SPEED (2-3s), BALANCED (4-6s), or QUALITY (10-16s)
# SPEED: Disables OCR, limits captions to 15, uses simple prompts
# BALANCED: Selective OCR, limits captions to 25, detailed prompts (RECOMMENDED)
# QUALITY: Full OCR, up to 100 captions, maximum accuracy
OMNIPARSER_PERFORMANCE_PROFILE=BALANCED

# Advanced settings (override profile defaults if needed)
# OMNIPARSER_ENABLE_OCR=true              # Enable OCR text detection (auto-determined by profile)
# OMNIPARSER_MAX_CAPTIONS=25              # Maximum elements to caption (auto-determined by profile)
# OMNIPARSER_CAPTION_PROMPT=DETAILED_CAPTION  # Caption detail level (auto-determined by profile)
# OMNIPARSER_BATCH_SIZE=32                # Caption batch size for MPS (32) or GPU (128)
# OMNIPARSER_MIN_CONFIDENCE=0.05          # Minimum confidence threshold
# OMNIPARSER_IOU_THRESHOLD=0.1            # Overlap removal threshold

# Device configuration (auto-detect recommended)
OMNIPARSER_DEVICE=auto
OMNIPARSER_MODEL_DTYPE=float16

# ==============================================================================
# Desktop Platform Selection
# ==============================================================================
# Choose desktop environment: 'linux' (default) or 'windows'
BYTEBOT_DESKTOP_PLATFORM=linux

# Desktop service URLs (configured automatically based on platform)
BYTEBOT_DESKTOP_LINUX_URL=http://bytebot-desktop:9990
BYTEBOT_DESKTOP_WINDOWS_URL=http://omnibox-adapter:5001

# Desktop VNC URLs for live desktop view in UI (auto-detected by UI server)
BYTEBOT_DESKTOP_LINUX_VNC_URL=http://bytebot-desktop:9990/websockify
BYTEBOT_DESKTOP_WINDOWS_VNC_URL=http://omnibox:8006/websockify
# Deprecated: Use platform-specific URLs above. Kept for backward compatibility.
BYTEBOT_DESKTOP_VNC_URL=http://bytebot-desktop:9990/websockify

# Desktop base URL for Computer Use API (auto-selected based on platform)
# This is used by bytebot-agent to route desktop control requests
BYTEBOT_DESKTOP_BASE_URL=http://bytebot-desktop:9990

# ==============================================================================
# OmniBox Configuration (Windows 11 VM)
# ==============================================================================
# Note: OmniBox is optional and uses docker-compose profile 'omnibox'
# Start with: docker compose --profile omnibox up -d

# VM Resources
OMNIBOX_RAM_SIZE=8G
OMNIBOX_CPU_CORES=4
OMNIBOX_DISK_SIZE=64G

# OmniBox API timeout
OMNIBOX_TIMEOUT=30000

# ==============================================================================
# LMStudio Local Model Server
# ==============================================================================
# LMStudio IP address and port
# Run ./scripts/setup-lmstudio.sh to auto-discover VLM models
LMSTUDIO_URL=http://192.168.4.112:1234

# Auto-discovery will:
# - Connect to LMStudio API
# - Find all Vision Language Models (VLMs)
# - Configure them in litellm-config.yaml
# - Make them available in the UI model picker
#
# VLMs are detected by model name patterns:
# - Contains: vl, vision, visual, multimodal
# - Known families: llava, qwen-vl, cogvlm, internvl, ui-tars

# ==============================================================================
# Ollama Local Model Server
# ==============================================================================
# Ollama server URL (usually runs locally)
# Run ./scripts/setup-ollama.sh to auto-discover VLM models
OLLAMA_URL=http://192.168.4.126:11434

# Auto-discovery will:
# - Connect to Ollama API
# - Find all Vision Language Models (VLMs)
# - Configure them in litellm-config.yaml
# - Make them available in the UI model picker
#
# VLMs are detected by model name patterns:
# - Known families: llava, qwen-vl, minicpm-v, bakllava
# - Contains: vision, vl, visual
#
# To download VLM models:
# - ollama pull llava:latest
# - ollama pull llava:13b
# - ollama pull llava:34b
# - ollama pull qwen2-vl:7b
# - ollama pull minicpm-v:latest
